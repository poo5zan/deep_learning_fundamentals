{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-4xgT39sQUj"
      },
      "source": [
        "Deep Learning Assignment 2 Pujan\n",
        "\n",
        "code reference: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "w4s_pul2sQUm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "get_device()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L4vTpqntB_Q",
        "outputId": "9dde0cb7-59f0-460e-c79c-f723e7715404"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "60000//10"
      ],
      "metadata": {
        "id": "ItqSK911B5ee",
        "outputId": "17816a9f-d2fc-4fcb-cfbd-7c633e760d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PujanNet(nn.Module):\n",
        "    def __init__(self, data_set_name, classes, batch_size, download_path, train_data_size, validation_data_size):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(5)\n",
        "    \n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        if data_set_name == \"cifar_10\":\n",
        "            dataset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=transform)\n",
        "            print('Length of Dataset ', len(dataset))\n",
        "\n",
        "            train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_data_size, validation_data_size])\n",
        "            print('Length of Train set ', len(train_dataset))\n",
        "            print('Length of Val set ', len(val_dataset))\n",
        "            self.trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "            self.valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "            testset = torchvision.datasets.CIFAR10(root=download_path, train=False, download=True, transform=transform)\n",
        "            print('Length of Testset ', len(testset))\n",
        "            self.testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def display_input_images(self, number_of_images):\n",
        "        # get some random training images\n",
        "        dataiter = iter(self.trainloader)\n",
        "        images, labels = dataiter.next()\n",
        "\n",
        "        # show images\n",
        "        imshow(torchvision.utils.make_grid(images))\n",
        "        # print labels\n",
        "        print(' '.join(f'{self.classes[labels[j]]:5s}' for j in range(number_of_images)))\n",
        "\n",
        "    def display_ground_truth_images(self, number_of_images):\n",
        "        dataiter = iter(self.testloader)\n",
        "        images, labels = dataiter.next()\n",
        "\n",
        "        # print images\n",
        "        imshow(torchvision.utils.make_grid(images))\n",
        "        print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def train(self, epoch_number, learning_rate, momentum, criterion, optimizer_name):\n",
        "        optimizer = optim.SGD(self.parameters(), lr= learning_rate, momentum=momentum)\n",
        "        # if optimizer_name == \"sgd\": \n",
        "\n",
        "        for epoch in range(epoch_number):  # loop over the dataset multiple times\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.trainloader, 0):\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                # inputs, labels = data\n",
        "                inputs, labels = data[0].to(get_device()), data[1].to(get_device())\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = self(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        print('Finished Training')\n",
        "\n",
        "    def predictions(self, input_images):\n",
        "        outputs = self(input_images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))\n",
        "\n",
        "    def accuracy(self):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "        with torch.no_grad():\n",
        "            for data in self.testloader:\n",
        "                # images, labels = data\n",
        "                images, labels = data[0].to(get_device()), data[1].to(get_device())\n",
        "                # calculate outputs by running images through the network\n",
        "                outputs = self(images)\n",
        "                # the class with the highest energy is what we choose as prediction\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "    def accuracy_in_each_class(self):\n",
        "    # prepare to count predictions for each class\n",
        "        correct_pred = {classname: 0 for classname in classes}\n",
        "        total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "        # again no gradients needed\n",
        "        with torch.no_grad():\n",
        "            for data in self.testloader:\n",
        "                # images, labels = data\n",
        "                images, labels = data[0].to(get_device()), data[1].to(get_device())\n",
        "                outputs = self(images)\n",
        "                _, predictions = torch.max(outputs, 1)\n",
        "                # collect the correct predictions for each class\n",
        "                for label, prediction in zip(labels, predictions):\n",
        "                    if label == prediction:\n",
        "                        correct_pred[classes[label]] += 1\n",
        "                    total_pred[classes[label]] += 1\n",
        "\n",
        "        # print accuracy for each class\n",
        "        for classname, correct_count in correct_pred.items():\n",
        "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "            print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "\n"
      ],
      "metadata": {
        "id": "FKedomWmtUGv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "pujan_net = PujanNet(data_set_name = \"cifar_10\", classes = classes, \n",
        "                     batch_size = 4, download_path = \"./data\", \n",
        "                     train_data_size = 40000, validation_data_size = 10000)\n",
        "pujan_net = pujan_net.to(get_device())\n",
        "\n",
        "pujan_net.train(epoch_number = 1, learning_rate = 0.01, momentum = 0.9, criterion = criterion, optimizer_name = \"sgd\")\n",
        "\n",
        "pujan_net.accuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RR5qjAVtgzf",
        "outputId": "e95c7f6a-152a-4b24-f055-13f82255074d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Length of Dataset  50000\n",
            "Length of Train set  40000\n",
            "Length of Val set  10000\n",
            "Files already downloaded and verified\n",
            "Length of Testset  10000\n",
            "[1,  2000] loss: 2.078\n",
            "[1,  4000] loss: 1.954\n",
            "[1,  6000] loss: 1.905\n",
            "[1,  8000] loss: 1.939\n",
            "[1, 10000] loss: 1.937\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 27 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiments\n",
        "# selecting the model\n",
        "# 1. CNN Architectures (How to choose CNN architecture ????)\n",
        "# 1.1. Number of Layers ??\n",
        "\n",
        "# Input data enhancements\n",
        "# 5. Data augmentation\n",
        "\n",
        "# Hyper parameters\n",
        "# 1. Loss\n",
        "# 2. Optimizer\n",
        "# 3. Epoch\n",
        "# 4. Learning rate\n",
        "# 5. Batch size\n",
        "\n"
      ],
      "metadata": {
        "id": "iJDaeTxz84-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jenw8USp7rb1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}