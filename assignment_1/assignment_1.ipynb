{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1: Predict diabetes using Perceptron\n",
    "Student: Pujan Maharjan (a1863495)\n",
    "Course: Deep Learning Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "# !pip install ipywidgets\n",
    "# !pip install seaborn\n",
    "# !pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand Data\n",
    "# filename = \"diabetes.txt\"\n",
    "filename = \"diabetes_scale.txt\"\n",
    "X, y = load_svmlight_file(filename)\n",
    "X = X.toarray()\n",
    "# reshape y from (768,) to (768,1)\n",
    "y = y.reshape(-1, 1)\n",
    "print('X : ', X.shape)\n",
    "print('Y : ', y.shape)\n",
    "all_data = np.append(X, y, axis=1)\n",
    "print('all_data Shape ', all_data.shape)\n",
    "columns = ['Pregnancies', 'Glucose', 'Blood Pressure', 'Skin Thickness', 'Insulin', 'BMI', 'Diabetes Pedigree', 'Age', 'Output']\n",
    "df = pd.DataFrame(all_data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data = df.describe().T\n",
    "desc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data[[\"mean\", \"std\", \"min\", \"max\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = df.hist(figsize=(6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Output'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot = scatter_matrix(df, figsize=(15,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self, \n",
    "        file_path, \n",
    "        weights, \n",
    "        loss_function_name, \n",
    "        learning_rate, \n",
    "        epoch,\n",
    "        add_bias = False) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.weights = weights\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.add_bias = add_bias\n",
    "        self.train_data = None\n",
    "        self.epochs = []\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []        \n",
    "        self.train_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "        self.train_validation_y_labels = ['Train', 'Validation']\n",
    "\n",
    "    def get_features_labels_from_file_data(self):\n",
    "        X, y = load_svmlight_file(self.file_path)\n",
    "        # convert X from scipy.sparce.csr.csr_matrix to numpy array\n",
    "        X = X.toarray()\n",
    "        # reshape y from (768,) to (768,1)\n",
    "        y = y.reshape(-1, 1)\n",
    "        return X,y\n",
    "\n",
    "    def predict(self, X):\n",
    "        np_sign_values = np.sign(np.dot(X, self.weights))\n",
    "        # numpy sign function returns -1, 0, 1,\n",
    "        # thus for 1 and 0, return 1\n",
    "        predictions_list = []\n",
    "        for np_sign_value in np_sign_values:\n",
    "            if np_sign_value >= 0:\n",
    "                predictions_list.append(1)\n",
    "            else:\n",
    "                predictions_list.append(-1)\n",
    "\n",
    "        return np.array(predictions_list).reshape(-1,1)\n",
    "\n",
    "    def zero_one_loss(self, X, y):\n",
    "        # print('zero one loss X ', X.shape, ', w , ',self.weights.shape)\n",
    "        xw = np.dot(X, self.weights)\n",
    "        losses = []\n",
    "        # if correct prediction, then loss = 0, else loss = 1\n",
    "        for i in range(len(y)):\n",
    "            indicator = y[i] * xw[i]\n",
    "            if indicator < 0:\n",
    "                losses.append(1)\n",
    "            else:\n",
    "                losses.append(0)\n",
    "\n",
    "        return np.array(losses).reshape(-1,1)\n",
    "\n",
    "    def perceptron_loss(self, X, y):        \n",
    "        x_w = np.dot(X, self.weights)\n",
    "        p_loss_mat = y * x_w\n",
    "        \n",
    "        #multiply by minus\n",
    "        p_loss_mat = -p_loss_mat\n",
    "        \n",
    "        zero_column_matrix = np.zeros(p_loss_mat.shape[0]).reshape(-1,1)\n",
    "        loss_matrix_with_zero_at_first_column = np.append(zero_column_matrix, p_loss_mat, axis=1)\n",
    "        \n",
    "        #find max in each row\n",
    "        perceptron_loss_values = np.amax(loss_matrix_with_zero_at_first_column, axis=1).reshape(-1,1)\n",
    "        \n",
    "        return perceptron_loss_values\n",
    "\n",
    "    def normal_loss(self, X, y):\n",
    "        normal_loss_values = self.predict(X) - y\n",
    "        return normal_loss_values\n",
    "\n",
    "    def l1_loss(self, X, y):\n",
    "        return abs(self.normal_loss(X,y))\n",
    "\n",
    "    def l2_loss(self, X, y):\n",
    "        return (self.predict(X) - y) ** 2\n",
    "\n",
    "    def add_bias_in_features(self, X_for_bias):\n",
    "        bias_X = np.ones((X_for_bias.shape[0],1))\n",
    "        X_for_bias = np.append(bias_X, X_for_bias, axis=1)\n",
    "        return X_for_bias\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        train_data = []\n",
    "        if self.add_bias:\n",
    "            X_train = self.add_bias_in_features(X_train)\n",
    "            X_val = self.add_bias_in_features(X_val)\n",
    "            bias_value = np.random.uniform(low=-.1,high=.1, size=1)\n",
    "            print('Bias Value ', bias_value)\n",
    "            bias_W = np.array([bias_value])\n",
    "            self.weights = np.append(bias_W, self.weights, axis=0)\n",
    "            \n",
    "        for epoch_number in range(self.epoch):\n",
    "            train_loss = None\n",
    "            validation_loss = None\n",
    "            train_accuracy = None\n",
    "          \n",
    "            if (self.loss_function_name == \"zero_one_loss\"):\n",
    "                train_loss = self.zero_one_loss(X_train, y_train)\n",
    "                validation_loss = self.zero_one_loss(X_val, y_val)  \n",
    "                                                                         \n",
    "            elif self.loss_function_name == \"perceptron_loss\":\n",
    "                train_loss = self.perceptron_loss(X_train, y_train)\n",
    "                validation_loss = self.perceptron_loss(X_val, y_val)\n",
    "\n",
    "            elif self.loss_function_name == \"l1_loss\":\n",
    "                train_loss = self.l1_loss(X_train, y_train)\n",
    "                validation_loss = self.l1_loss(X_val, y_val)\n",
    "\n",
    "            elif self.loss_function_name == \"l2_loss\":\n",
    "                train_loss = self.l2_loss(X_train, y_train)\n",
    "                validation_loss = self.l2_loss(X_val, y_val)\n",
    "\n",
    "            else:\n",
    "                raise \"Loss function error \" + self.loss_function_name\n",
    "                \n",
    "            yxlr = self.learning_rate * y_train * X_train * train_loss\n",
    "            yxlr_sum = np.sum(yxlr, axis=0).reshape(-1,1)                \n",
    "            self.weights = self.weights + yxlr_sum\n",
    "\n",
    "            train_accuracy = self.accuracy(X_train, y_train)\n",
    "            validation_accuracy = self.accuracy(X_val, y_val)\n",
    "            train_loss_sum = np.sum(train_loss) / X_train.shape[0]\n",
    "            validation_loss_sum = np.sum(validation_loss) / X_val.shape[0]\n",
    "\n",
    "            # print('Epoch ', epoch_number, ', Val accuracy: ', validation_accuracy, ', Val loss: ', validation_loss_sum)\n",
    "            self.epochs.append(epoch_number)\n",
    "            self.train_losses.append(train_loss_sum)\n",
    "            self.validation_losses.append(validation_loss_sum)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            self.validation_accuracies.append(validation_accuracy)\n",
    "            train_data.append({\n",
    "                'learning_rate': self.learning_rate, \n",
    "                'epoch': epoch_number, \n",
    "                'train_loss': train_loss_sum, \n",
    "                'val_loss': validation_loss_sum,\n",
    "                'val_accuracy': validation_accuracy,\n",
    "                'train_accuracy': train_accuracy})\n",
    "\n",
    "        # print('Training Completed')\n",
    "        self.train_data = train_data\n",
    "        return train_data\n",
    "\n",
    "    def accuracy(self, X_accuracy, y_accuracy):\n",
    "        predictions_for_accuracy = self.predict(X_accuracy)\n",
    "        accuracy_score_from_sk_learn = accuracy_score(y_accuracy, predictions_for_accuracy)\n",
    "        return accuracy_score_from_sk_learn\n",
    "\n",
    "    def split_train_validation_test(self, X, y, test_split_percentage):\n",
    "        # reference to split (train/validation/test):\n",
    "        #  https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "        print('test_split_percentage ', test_split_percentage)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "            test_size=test_split_percentage, \n",
    "            random_state=1, \n",
    "            shuffle=True,\n",
    "            # stratify=y\n",
    "            )\n",
    "        # 0.25 * 0.8 = 0.2\n",
    "        val_split_percentage = test_split_percentage / (1 - test_split_percentage)\n",
    "        # print('val_split_percentage ', val_split_percentage)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "            test_size=val_split_percentage, \n",
    "            random_state=1, \n",
    "            shuffle=True,\n",
    "            # stratify=y_train\n",
    "            )\n",
    "\n",
    "        print('Train: ', X_train.shape[0], ', Val: ', X_val.shape[0], ', Test: ', X_test.shape[0])\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def plot_line_graphs(self, x_values, y_values_list, y_labels, title):\n",
    "        for y_values, y_label in zip(y_values_list, y_labels):\n",
    "            plt.plot(x_values, y_values, label = y_label)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_curves(self, title_suffix):        \n",
    "        y_values_list = [self.train_losses, self.validation_losses]\n",
    "        self.plot_line_graphs(self.epochs, y_values_list, self.train_validation_y_labels, 'Loss Curves: ' + title_suffix)\n",
    "\n",
    "    def plot_accuracy_curves(self, title_suffix):\n",
    "        y_values_list = [self.train_accuracies, self.validation_accuracies]\n",
    "        self.plot_line_graphs(self.epochs, y_values_list, self.train_validation_y_labels, 'Accuracy Curves: ' + title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# 1. Input Size, split sizes\n",
    "split_sizes = [.1,.15,.2]\n",
    "# 2. Weight Random seed\n",
    "random_seeds = [0, 100, 200]\n",
    "# 3. Weight pairs\n",
    "weight_pairs = [(-1,1), (0,1), (-0.5, 0.5)]\n",
    "# 4. bias\n",
    "bias_values = [False, True]\n",
    "# 5. loss functions\n",
    "loss_functions = ['zero_one_loss', 'perceptron_loss', 'l1_loss', 'l2_loss']\n",
    "# 6. Learning rate\n",
    "learning_rates = [1,0.1,0.001,0.0001]\n",
    "# 7. Epoch\n",
    "epochs = [10,20,30,40,50,60,70]\n",
    "default_file_path = 'diabetes_scale.txt'\n",
    "def train_model(weight_pair, loss_function_name, learning_rate, epoch, split_size,\n",
    "    bias_value):\n",
    "    low, high = weight_pair\n",
    "    weights = np.random.uniform(low=low, high = high, size=8).reshape(-1,1)\n",
    "    perceptron = Perceptron( \n",
    "        file_path=default_file_path,\n",
    "        weights=weights, \n",
    "        loss_function_name=loss_function_name, \n",
    "        learning_rate=learning_rate, \n",
    "        epoch=epoch,\n",
    "        add_bias=bias_value)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(\n",
    "        X, y, split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    perceptron.plot_loss_curves(default_file_path)\n",
    "    perceptron.plot_accuracy_curves(default_file_path)\n",
    "    # print('train data ', train_data)\n",
    "    last_train_data = [td for td in train_data if td['epoch'] == epoch - 1][0]\n",
    "    # print('lt ', last_train_data)\n",
    "    return {\n",
    "            'split_size': split_size,\n",
    "            'random_seed': random_seed,\n",
    "            'weight_pair': weight_pair,\n",
    "            'bias_value': bias_value,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch': epoch,\n",
    "            'train_accuracy': last_train_data['train_accuracy'],\n",
    "            'val_accuracy': last_train_data['val_accuracy'],\n",
    "            'train_loss': last_train_data['train_loss'],\n",
    "            'val_loss': last_train_data['val_loss']\n",
    "            }\n",
    "\n",
    "results = []\n",
    "for split_size in split_sizes:\n",
    "    for random_seed in random_seeds:\n",
    "        for weight_pair in weight_pairs:\n",
    "            for bias_value in bias_values:\n",
    "                for loss_function_name in loss_functions:\n",
    "                    for learning_rate in learning_rates:\n",
    "                        for epoch in epochs:\n",
    "                            # Experiment\n",
    "                            print('Experiment ',\n",
    "                                ', split_size: ', split_size, \n",
    "                                ', random_seed: ', random_seed,\n",
    "                                ', weight_pair: ', weight_pair,\n",
    "                                ', bias_value: ', bias_value,\n",
    "                                ', loss_function_name: ', loss_function_name,\n",
    "                                ', learning_rate ', learning_rate,\n",
    "                                ', epoch: ', epoch)\n",
    "                            result = train_model(\n",
    "                                weight_pair, \n",
    "                                loss_function_name, \n",
    "                                learning_rate, \n",
    "                                epoch, \n",
    "                                split_size,\n",
    "                                bias_value)\n",
    "\n",
    "                            results.append(result)\n",
    "\n",
    "\n",
    "                        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "# 1. Different split sizes\n",
    "np.random.seed(0)\n",
    "weights = np.random.rand(8,1)\n",
    "for split_size in [.1,.15,.2]:\n",
    "    perceptron = Perceptron( \n",
    "        file_path=default_file_path,\n",
    "        weights=weights, \n",
    "        loss_function_name=\"zero_one_loss\", \n",
    "        learning_rate=default_learning_rate, \n",
    "        epoch=default_epoch)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    perceptron.plot_loss_curves(default_file_path)\n",
    "    perceptron.plot_accuracy_curves(default_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result\n",
    "# split of 80, 10, 10 is optimal\n",
    "optimal_split_size = 0.1\n",
    "optimal_split_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Weights\n",
    "# low,high pair\n",
    "#  [(-1,1), (0,1), (-2,2),(-0.5,0.5),(0,0)]\n",
    "# weights_pairs = [(-1,1), (0,1), (-2,2),(-0.5,0.5),(0,0)]\n",
    "random_seeds = range(0,400,100)\n",
    "for random_seed in random_seeds:\n",
    "    np.random.seed(random_seed)\n",
    "    # low, high = weight_pair\n",
    "    # weights = np.random.uniform(low=low, high = high, size=8).reshape(-1,1)\n",
    "    # weights = np.random.normal(size=8).reshape(-1,1)\n",
    "    weights = np.random.rand(8,1)\n",
    "    perceptron = Perceptron(\n",
    "        file_path='diabetes_scale.txt',\n",
    "        weights=weights, \n",
    "        loss_function_name=\"zero_one_loss\", \n",
    "        learning_rate=default_learning_rate, \n",
    "        epoch=default_epoch)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, optimal_split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    title_suffix = 'Random seed: ' + str(random_seed)\n",
    "    perceptron.plot_loss_curves(title_suffix)\n",
    "    perceptron.plot_accuracy_curves(title_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Weights distribution\n",
    "# low,high pair\n",
    "#  [(-1,1), (0,1), (-2,2),(-0.5,0.5),(0,0)]\n",
    "weights_pairs = [(-1,1), (0,1), (-2,2),(-0.5,0.5),(0,0)]\n",
    "np.random.seed(0)\n",
    "for weight_pair in weights_pairs:\n",
    "    low, high = weight_pair\n",
    "    weights = np.random.uniform(low=low, high = high, size=8).reshape(-1,1)\n",
    "    perceptron = Perceptron(\n",
    "        file_path=default_file_path,\n",
    "        weights=weights, \n",
    "        loss_function_name=\"zero_one_loss\", \n",
    "        learning_rate=default_learning_rate, \n",
    "        epoch=default_epoch)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, optimal_split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    title_suffix = 'Weight pair: ' + str(weight_pair)\n",
    "    perceptron.plot_loss_curves(title_suffix)\n",
    "    perceptron.plot_accuracy_curves(title_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "optimal_weights = np.random.uniform(low=-0.5, high = 0.5, size=8).reshape(-1,1)\n",
    "optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "# 4. Bias\n",
    "np.random.seed(0)\n",
    "optimal_weights = np.random.uniform(low=-0.5, high = 0.5, size=8).reshape(-1,1)\n",
    "perceptron = Perceptron(\n",
    "    file_path=default_file_path,\n",
    "    weights=optimal_weights, \n",
    "    loss_function_name=\"zero_one_loss\", \n",
    "    learning_rate=default_learning_rate, \n",
    "    epoch=default_epoch,\n",
    "    add_bias=True)\n",
    "X, y = perceptron.get_features_labels_from_file_data()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, optimal_split_size)\n",
    "train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "perceptron.plot_loss_curves('Bias = True')\n",
    "perceptron.plot_accuracy_curves('Bias = True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss functions\n",
    "loss_function_names = [\"zero_one_loss\", \"perceptron_loss\", \"normal_absolute_loss\", \"mean_squared_error_loss\"]\n",
    "\n",
    "for loss_function_name in loss_function_names:\n",
    "    np.random.seed(0)\n",
    "    # optimal_weights = np.random.uniform(low=-0.5, high = 0.5, size=8).reshape(-1,1)\n",
    "    optimal_weights = np.random.rand()\n",
    "    perceptron = Perceptron(\n",
    "        file_path=default_file_path,\n",
    "        weights=optimal_weights, \n",
    "        loss_function_name=loss_function_name, \n",
    "        learning_rate=default_learning_rate, \n",
    "        epoch=default_epoch)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, optimal_split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    title_suffix = 'Loss Function name: ' + str(loss_function_name)\n",
    "    perceptron.plot_loss_curves(title_suffix)\n",
    "    perceptron.plot_accuracy_curves(title_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Learning Rate\n",
    "\n",
    "for learning_rate in [1,0.1,0.01,0.001,0.0001]:\n",
    "    np.random.seed(0)\n",
    "    optimal_weights = np.random.uniform(low=-0.5, high = 0.5, size=8).reshape(-1,1)\n",
    "    perceptron = Perceptron(\n",
    "        file_path=default_file_path,\n",
    "        weights=optimal_weights, \n",
    "        loss_function_name=\"zero_one_loss\", \n",
    "        learning_rate=learning_rate, \n",
    "        epoch=default_epoch)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, optimal_split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    # test_accuracy = perceptron.accuracy(X_test, y_test)\n",
    "    # print('Learning Rate = ', learning_rate, ' test_accuracy = ', test_accuracy)\n",
    "    title_suffix = 'Learning Rate ' + str(learning_rate)\n",
    "    perceptron.plot_loss_curves(title_suffix)\n",
    "    perceptron.plot_accuracy_curves(title_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_learning_rate = 0.1\n",
    "optimal_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Epoch\n",
    "\n",
    "for epoch in range(10, 100, 10):\n",
    "    np.random.seed(0)\n",
    "    optimal_weights = np.random.uniform(low=-0.5, high = 0.5, size=8).reshape(-1,1)\n",
    "    perceptron = Perceptron(\n",
    "        file_path=default_file_path,\n",
    "        weights=optimal_weights, \n",
    "        loss_function_name=\"zero_one_loss\", \n",
    "        learning_rate=default_learning_rate, \n",
    "        epoch=epoch)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y, optimal_split_size)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    # test_accuracy = perceptron.accuracy(X_test, y_test)\n",
    "    # print('Epoch = ', epoch, ' test_accuracy = ', test_accuracy)\n",
    "    title_suffix = 'Epoch ' + str(epoch)\n",
    "    perceptron.plot_loss_curves(title_suffix)\n",
    "    perceptron.plot_accuracy_curves(title_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x1 = np.arange(15).reshape(3,5)\n",
    "y1 = np.array([1,-1,1]).reshape(3,1)\n",
    "w1 = np.random.rand(5,1)\n",
    "# print(x.shape, y.shape, w.shape)\n",
    "\n",
    "# zero one loss\n",
    "def zero_one_indicator_function(yi, xi, w):\n",
    "    xi_dot_wi = np.dot(xi, w)\n",
    "    print('xi_dot_wi value ', xi_dot_wi)\n",
    "    yi_xi_w = yi * xi_dot_wi\n",
    "    print('yi_xi_w value ', yi_xi_w)\n",
    "    if yi_xi_w < 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def zero_one_loss(y, x, w):\n",
    "    zero_one_losses = []\n",
    "    for yi, xi in zip(y, x):\n",
    "        print('yi value ', yi)\n",
    "        print('xi value ', xi)\n",
    "        print('w value ', w)\n",
    "        zero_one = zero_one_indicator_function(yi, xi, w)\n",
    "        zero_one_losses.append(zero_one)\n",
    "\n",
    "    return zero_one_losses\n",
    "\n",
    "loss = zero_one_loss(y1, x1, w1)\n",
    "print(' loss values ', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "# 1. Different Files (Without scaling vs scaled)\n",
    "np.random.seed(0)\n",
    "# weights = np.random.uniform(low = -1, high = 1, size=8).reshape(-1,1)\n",
    "# weights = np.random.normal(size=8).reshape(-1,1)\n",
    "weights = np.random.rand(8,1)\n",
    "for file_path in ['diabetes_scale.txt','diabetes.txt']:\n",
    "    perceptron = Perceptron(\n",
    "        file_path=file_path,\n",
    "        weights=weights, \n",
    "        loss_function_name=\"zero_one_loss\", \n",
    "        # loss_function_name=\"perceptron_loss\",\n",
    "        # loss_function_name=\"normal_loss\",\n",
    "        # loss_function_name=\"normal_absolute_loss\",\n",
    "        # loss_function_name=\"mean_squared_error_loss\",\n",
    "        learning_rate=0.01, \n",
    "        epoch=20)\n",
    "    X, y = perceptron.get_features_labels_from_file_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = perceptron.split_train_validation_test(X, y)\n",
    "    train_data = perceptron.train(X_train, y_train, X_val, y_val)\n",
    "    test_accuracy = perceptron.accuracy(X_test, y_test)\n",
    "    print('File = ', file_path, ' test_accuracy = ', test_accuracy)\n",
    "    # train_data_pd = pd.DataFrame(train_data)\n",
    "    # print(train_data_pd)\n",
    "    perceptron.plot_loss_curves(file_path)\n",
    "    perceptron.plot_accuracy_curves(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "\n",
    "res.append({\n",
    "    'lr': 1,\n",
    "    'loss': 'loss_1',\n",
    "    'val_acc': 5\n",
    "})\n",
    "\n",
    "res.append({\n",
    "    'lr': 1,\n",
    "    'loss': 'loss_2',\n",
    "    'val_acc': 5\n",
    "})\n",
    "\n",
    "res.append({\n",
    "    'lr': 1,\n",
    "    'val_acc': 50\n",
    "})\n",
    "\n",
    "print(res)\n",
    "resdf = pd.DataFrame(res)\n",
    "resdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b5451533b344355a501597b588174e3182b6d0803b6c4a8ab339d901650e8cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
